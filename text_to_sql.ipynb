{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text-to-SQL Lab \n",
        "\n",
        "**Last updated:** 2025-10-23\n",
        "\n",
        "This notebook is designed for a hands-on lab. It starts from a working\n",
        "Text-to-SQL pipeline and adds structured guidance, exercises, and safety checks.\n",
        "Students can explore prompting strategies, schema linking, validation, and evaluation.\n",
        "\n",
        "## Learning objectives\n",
        "By the end of the lab, students will be able to:\n",
        "- Explain the basic Text-to-SQL pipeline and its risks.\n",
        "- Inspect a SQLite schema and design prompts that condition the model on schema.\n",
        "- Generate and execute SQL safely (read-only; parameterized where relevant).\n",
        "- Evaluate query correctness with simple heuristics and unit-style tests.\n",
        "- Modularize code into files to build a small FastAPI app.\n",
        "\n",
        "## Prerequisites\n",
        "- Python 3.10+\n",
        "- A local SQLite database (provided or generated in this notebook)\n",
        "- An LLM provider key (e.g., OpenAI API key) available as an environment variable\n",
        "\n",
        "> Tip: **Never** run untrusted SQL that can mutate the database. In this lab we restrict to `SELECT` queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "1. Create a virtual environment and install dependencies (example):\n",
        "   ```bash\n",
        "   python -m venv .venv && source .venv/bin/activate\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "2. Set your API key (example for OpenAI) in a `.env` file:\n",
        "   ```bash\n",
        "   OPENAI_API_KEY=\"sk-...\"\n",
        "   ```\n",
        "\n",
        "3. Run the first section to create / connect to the sample SQLite database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab map\n",
        "\n",
        "1. **Data & schema**: create or connect to SQLite; inspect tables.\n",
        "2. **Baseline text→SQL**: prompt LLM, generate SQL, run the query.\n",
        "3. **Safety pass**: restrict to `SELECT`, block DDL/DML, and validate SQL.\n",
        "4. **Prompt engineering**: add schema, few-shots, and constraints.\n",
        "5. **Evaluation**: define expected outputs, compare, and log failures.\n",
        "6. **Refactor**: extract functions and move into a FastAPI app."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a42bd523",
      "metadata": {},
      "source": [
        "Based on: Creating a Text to SQL App with OpenAI + FastAPI + SQLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b38d4f9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.orm import Session\n",
        "import os\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from sqlalchemy import inspect\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "from sqlalchemy import create_engine, inspect\n",
        "from sqlalchemy.engine import Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d93cde57",
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffcfeb01",
      "metadata": {},
      "source": [
        "## 📘 Step 1 — Create & seed the demo database\n",
        "\n",
        "**Why this matters**\n",
        "- A consistent, realistic schema lets you practice schema grounding and query reasoning.\n",
        "- Seeding deterministic data enables repeatable evaluation later.\n",
        "- We’ll use this DB both in-notebook and later when we refactor into a FastAPI app.\n",
        "\n",
        "**What’s provided**\n",
        "- Code to (re)create `demo.db` and populate `customers`, `products`, `orders`, `order_items`, `payments`.\n",
        "\n",
        "**Students do**\n",
        "- [ ] Inspect the schema (tables, columns, types) via PRAGMA queries and helper functions.  \n",
        "- [ ] Produce short **schema bullets** (1–2 lines/table) to feed into the prompt later.\n",
        "\n",
        "**Checks**\n",
        "- You can list the tables and see sensible row counts.\n",
        "- `PRAGMA foreign_key_list(…)` shows expected relationships.\n",
        "\n",
        "**Stretch**\n",
        "- Add an integrity check: verify that `orders.total` equals the sum of `order_items.quantity * unit_price` for each order.\n",
        "- Add a view `order_totals` and compare it to the stored `orders.total`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ef6e3dd3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created C:\\git_repository\\TAG-Exercise\\demo.db\n"
          ]
        }
      ],
      "source": [
        "SCHEMA_SQL = '''\n",
        "DROP TABLE IF EXISTS order_items;\n",
        "DROP TABLE IF EXISTS orders;\n",
        "DROP TABLE IF EXISTS payments;\n",
        "DROP TABLE IF EXISTS products;\n",
        "DROP TABLE IF EXISTS customers;\n",
        "\n",
        "CREATE TABLE customers (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    name TEXT NOT NULL,\n",
        "    country TEXT,\n",
        "    signup_date DATE\n",
        ");\n",
        "\n",
        "CREATE TABLE products (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    name TEXT NOT NULL,\n",
        "    category TEXT,\n",
        "    price REAL\n",
        ");\n",
        "\n",
        "CREATE TABLE orders (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    customer_id INTEGER,\n",
        "    order_date DATE,\n",
        "    total REAL,\n",
        "    FOREIGN KEY (customer_id) REFERENCES customers(id)\n",
        ");\n",
        "\n",
        "CREATE TABLE order_items (\n",
        "    order_id INTEGER,\n",
        "    product_id INTEGER,\n",
        "    quantity INTEGER,\n",
        "    unit_price REAL,\n",
        "    PRIMARY KEY (order_id, product_id),\n",
        "    FOREIGN KEY (order_id) REFERENCES orders(id),\n",
        "    FOREIGN KEY (product_id) REFERENCES products(id)\n",
        ");\n",
        "\n",
        "CREATE TABLE payments (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    order_id INTEGER,\n",
        "    payment_date DATE,\n",
        "    amount REAL,\n",
        "    method TEXT,\n",
        "    FOREIGN KEY (order_id) REFERENCES orders(id)\n",
        ");\n",
        "\n",
        "INSERT INTO customers (id, name, country, signup_date) VALUES\n",
        " (1,'Alice','USA','2024-01-05'),\n",
        " (2,'Bob','UK','2024-03-10'),\n",
        " (3,'Choi','KR','2024-06-22'),\n",
        " (4,'Dara','ID','2025-01-15');\n",
        "\n",
        "INSERT INTO products (id, name, category, price) VALUES\n",
        " (1,'Laptop Pro','Electronics',1500.00),\n",
        " (2,'Noise-Canceling Headphones','Electronics',300.00),\n",
        " (3,'Standing Desk','Furniture',450.00),\n",
        " (4,'Ergonomic Chair','Furniture',250.00),\n",
        " (5,'Monitor 27\"', 'Electronics',350.00);\n",
        "\n",
        "INSERT INTO orders (id, customer_id, order_date, total) VALUES\n",
        " (1,1,'2025-02-01',1850.00),\n",
        " (2,2,'2025-02-03',600.00),\n",
        " (3,3,'2025-02-05',350.00),\n",
        " (4,1,'2025-02-07',450.00);\n",
        "\n",
        "INSERT INTO order_items (order_id, product_id, quantity, unit_price) VALUES\n",
        " (1,1,1,1500.00),\n",
        " (1,2,1,300.00),\n",
        " (1,5,1,350.00),\n",
        " (2,3,1,450.00),\n",
        " (2,4,1,250.00),\n",
        " (3,5,1,350.00),\n",
        " (4,3,1,450.00);\n",
        "\n",
        "INSERT INTO payments (id, order_id, payment_date, amount, method) VALUES\n",
        " (1,1,'2025-02-01',1850.00,'Credit Card'),\n",
        " (2,2,'2025-02-03',600.00,'PayPal'),\n",
        " (3,3,'2025-02-05',350.00,'Credit Card'),\n",
        " (4,4,'2025-02-07',450.00,'Bank Transfer');\n",
        "'''\n",
        "DB_PATH = Path(\"demo.db\")\n",
        "if DB_PATH.exists():\n",
        "    DB_PATH.unlink()\n",
        "with sqlite3.connect(DB_PATH) as conn:\n",
        "    conn.executescript(SCHEMA_SQL)\n",
        "print(\"Created\", DB_PATH.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f32a2bbd",
      "metadata": {},
      "source": [
        "### 🧭 Step 1 — Schema inspection tasks\n",
        "\n",
        "- Use:\n",
        "  - `SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';`\n",
        "  - `PRAGMA table_info(<table>);`\n",
        "  - `PRAGMA foreign_key_list(<table>);`\n",
        "- Summarize each table in 1–2 lines, e.g.  \n",
        "  `orders(id PK, customer_id INTEGER, order_date DATE, total REAL; customer_id → customers.id)`\n",
        "- Save your bullets in a Python list (e.g., `schema_bullets = [...]`) to reuse in prompts.\n",
        "- Optional checks:\n",
        "  - Count rows per table.\n",
        "  - Verify that every `order_items.order_id` exists in `orders.id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "54e26706",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tables: ['customers', 'products', 'orders', 'order_items', 'payments']\n",
            "--------------------\n",
            "\n",
            "[customers]\n",
            "Columns: [{'name': 'id', 'type': 'INTEGER', 'pk': 1, 'notnull': 0, 'dflt_value': None}, {'name': 'name', 'type': 'TEXT', 'pk': 0, 'notnull': 1, 'dflt_value': None}, {'name': 'country', 'type': 'TEXT', 'pk': 0, 'notnull': 0, 'dflt_value': None}, {'name': 'signup_date', 'type': 'DATE', 'pk': 0, 'notnull': 0, 'dflt_value': None}]\n",
            "FKs: []\n",
            "\n",
            "[products]\n",
            "Columns: [{'name': 'id', 'type': 'INTEGER', 'pk': 1, 'notnull': 0, 'dflt_value': None}, {'name': 'name', 'type': 'TEXT', 'pk': 0, 'notnull': 1, 'dflt_value': None}, {'name': 'category', 'type': 'TEXT', 'pk': 0, 'notnull': 0, 'dflt_value': None}, {'name': 'price', 'type': 'REAL', 'pk': 0, 'notnull': 0, 'dflt_value': None}]\n",
            "FKs: []\n",
            "\n",
            "[orders]\n",
            "Columns: [{'name': 'id', 'type': 'INTEGER', 'pk': 1, 'notnull': 0, 'dflt_value': None}, {'name': 'customer_id', 'type': 'INTEGER', 'pk': 0, 'notnull': 0, 'dflt_value': None}, {'name': 'order_date', 'type': 'DATE', 'pk': 0, 'notnull': 0, 'dflt_value': None}, {'name': 'total', 'type': 'REAL', 'pk': 0, 'notnull': 0, 'dflt_value': None}]\n",
            "FKs: [{'from': 'customer_id', 'to_table': 'customers', 'to_col': 'id'}]\n",
            "\n",
            "[order_items]\n",
            "Columns: [{'name': 'order_id', 'type': 'INTEGER', 'pk': 1, 'notnull': 0, 'dflt_value': None}, {'name': 'product_id', 'type': 'INTEGER', 'pk': 2, 'notnull': 0, 'dflt_value': None}, {'name': 'quantity', 'type': 'INTEGER', 'pk': 0, 'notnull': 0, 'dflt_value': None}, {'name': 'unit_price', 'type': 'REAL', 'pk': 0, 'notnull': 0, 'dflt_value': None}]\n",
            "FKs: [{'from': 'product_id', 'to_table': 'products', 'to_col': 'id'}, {'from': 'order_id', 'to_table': 'orders', 'to_col': 'id'}]\n",
            "\n",
            "[payments]\n",
            "Columns: [{'name': 'id', 'type': 'INTEGER', 'pk': 1, 'notnull': 0, 'dflt_value': None}, {'name': 'order_id', 'type': 'INTEGER', 'pk': 0, 'notnull': 0, 'dflt_value': None}, {'name': 'payment_date', 'type': 'DATE', 'pk': 0, 'notnull': 0, 'dflt_value': None}, {'name': 'amount', 'type': 'REAL', 'pk': 0, 'notnull': 0, 'dflt_value': None}, {'name': 'method', 'type': 'TEXT', 'pk': 0, 'notnull': 0, 'dflt_value': None}]\n",
            "FKs: [{'from': 'order_id', 'to_table': 'orders', 'to_col': 'id'}]\n",
            "--------------------\n",
            "\n",
            "Schema bullets:\n",
            "- customers(id INTEGER PK, name TEXT, country TEXT, signup_date DATE)\n",
            "- products(id INTEGER PK, name TEXT, category TEXT, price REAL)\n",
            "- orders(id INTEGER PK, customer_id INTEGER, order_date DATE, total REAL, customer_id → customers.id)\n",
            "- order_items(order_id INTEGER PK, product_id INTEGER PK, quantity INTEGER, unit_price REAL, product_id → products.id, order_id → orders.id)\n",
            "- payments(id INTEGER PK, order_id INTEGER, payment_date DATE, amount REAL, method TEXT, order_id → orders.id)\n"
          ]
        }
      ],
      "source": [
        "# Step 1 — Schema inspection helpers (STUDENTS)\n",
        "\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "\n",
        "DB_PATH = Path(\"demo.db\")\n",
        "\n",
        "# === TODO (students) ===\n",
        "# Implement the helpers below to explore the schema and produce short, prompt-ready bullets.\n",
        "\n",
        "def list_tables(conn):\n",
        "    \"\"\"\n",
        "    Return a list of table names (excluding SQLite internal tables).\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "        SELECT name\n",
        "        FROM sqlite_master\n",
        "        WHERE type = 'table'\n",
        "          AND name NOT LIKE 'sqlite_%';\n",
        "    \"\"\"\n",
        "    \n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query)\n",
        "    tables = [row[0] for row in cursor.fetchall()]\n",
        "    cursor.close()\n",
        "    \n",
        "    return tables\n",
        "\n",
        "def describe_table(conn, table):\n",
        "    \"\"\"\n",
        "    Return a list of dicts: [{'name': 'col', 'type': 'TEXT', 'pk': 0, 'notnull': 0, 'dflt_value': None}, ...]\n",
        "    \"\"\"\n",
        "    query = f\"PRAGMA table_info({table});\"\n",
        "    \n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query)\n",
        "    columns = [\n",
        "        {\n",
        "            'name': row[1],\n",
        "            'type': row[2],\n",
        "            'pk': row[5],\n",
        "            'notnull': row[3],\n",
        "            'dflt_value': row[4]\n",
        "        }\n",
        "        for row in cursor.fetchall()\n",
        "    ]\n",
        "    cursor.close()\n",
        "\n",
        "    return columns\n",
        "\n",
        "def foreign_keys(conn, table):\n",
        "    \"\"\"\n",
        "    Return a list of foreign key dicts with 'from', 'to_table', 'to_col'.\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: use PRAGMA foreign_key_list(table);\n",
        "    query = f\"PRAGMA foreign_key_list({table});\"\n",
        "\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query)\n",
        "    fks = [\n",
        "        {\n",
        "            'from': row[3],\n",
        "            'to_table': row[2],\n",
        "            'to_col': row[4]\n",
        "        }\n",
        "        for row in cursor.fetchall()\n",
        "    ]\n",
        "    cursor.close()\n",
        "\n",
        "    return fks\n",
        "\n",
        "    \n",
        "\n",
        "def schema_bullets(conn):\n",
        "    \"\"\"\n",
        "    Produce 1–2 line summaries per table to condition the LLM prompt.\n",
        "    Example bullet: \"orders(id PK, customer_id → customers.id, order_date DATE, total REAL)\"\n",
        "    \"\"\"\n",
        "    bullets = []\n",
        "    for table in list_tables(conn):\n",
        "        columns = describe_table(conn, table)\n",
        "        fks = foreign_keys(conn, table)\n",
        "\n",
        "        col_bullets = []\n",
        "        for col in columns:\n",
        "            col_desc = f\"{col['name']} {col['type']}\"\n",
        "            if col['pk']:\n",
        "                col_desc += \" PK\"\n",
        "            col_bullets.append(col_desc)\n",
        "\n",
        "        fk_bullets = []\n",
        "        for fk in fks:\n",
        "            fk_bullets.append(f\"{fk['from']} → {fk['to_table']}.{fk['to_col']}\")\n",
        "\n",
        "        bullets.append(f\"{table}({', '.join(col_bullets + fk_bullets)})\")\n",
        "\n",
        "    return bullets\n",
        "\n",
        "# --- Checks ---\n",
        "with sqlite3.connect(DB_PATH) as conn:\n",
        "    print(\"Tables:\", list_tables(conn))\n",
        "    print(\"--------------------\")\n",
        "    for t in list_tables(conn):\n",
        "        print(f\"\\n[{t}]\")\n",
        "        print(\"Columns:\", describe_table(conn, t))\n",
        "        print(\"FKs:\", foreign_keys(conn, t))\n",
        "    print(\"--------------------\")\n",
        "    print(\"\\nSchema bullets:\")\n",
        "    for b in schema_bullets(conn):\n",
        "        print(\"-\", b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587ccb9b",
      "metadata": {},
      "source": [
        "# 📘 Step 1 — Create & Seed the Demo Database\n",
        "\n",
        "This section handles the crucial step of **Database Grounding**. We create a consistent SQLite database and establish a robust, **read-only connection**. A read-only connection is a vital **security measure** to ensure the LLM cannot execute destructive SQL commands (like `DROP TABLE` or `DELETE`).\n",
        "\n",
        "### 1.1 Database Setup (Provided Code)\n",
        "\n",
        "The code below performs three main actions:\n",
        "\n",
        "1.  **Engine Creation:** It uses **SQLAlchemy** to create a database engine. We choose SQLAlchemy over the standard `sqlite3` library because it provides a uniform way to handle database URIs and connection pooling, which is essential for our final step: building a scalable FastAPI application.\n",
        "2.  **Security Measures:** The connection URI includes `mode=ro&uri=true`, and we add a layer of **defense-in-depth** by executing `PRAGMA query_only = ON;` to explicitly prevent write operations.\n",
        "3.  **Schema Extraction:** It uses SQLAlchemy's `inspect` to automatically generate a simple `CREATE TABLE` string (`SCHEMA_STR`). This string is the **baseline schema prompt** fed to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5668c03c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CREATE TABLE customers (id, name, country, signup_date);\n",
            "CREATE TABLE order_items (order_id, product_id, quantity, unit_price);\n",
            "CREATE TABLE orders (id, customer_id, order_date, total);\n",
            "CREATE TABLE payments (id, order_id, payment_date, amount, method);\n",
            "CREATE TABLE products (id, name, category, price);\n"
          ]
        }
      ],
      "source": [
        "DB_URI = f\"sqlite:///file:{DB_PATH.resolve()}?mode=ro&uri=true\"\n",
        "engine: Engine = create_engine(DB_URI, connect_args={\"uri\": True})\n",
        "\n",
        "# Defense-in-depth: block writes on the connection\n",
        "with engine.connect() as conn:\n",
        "    try:\n",
        "        conn.exec_driver_sql(\"PRAGMA query_only = ON;\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Build a short schema summary to feed the LLM\n",
        "insp = inspect(engine)\n",
        "SCHEMA_STR = \"\\n\".join(\n",
        "    f\"CREATE TABLE {t} ({', '.join(c['name'] for c in insp.get_columns(t))});\"\n",
        "    for t in insp.get_table_names()\n",
        ")\n",
        "schema = SCHEMA_STR\n",
        "print(SCHEMA_STR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815aaea6",
      "metadata": {},
      "source": [
        "### 📝 Baseline Text-to-SQL Generation\n",
        "\n",
        "Now that we have the schema (`SCHEMA_STR`) and a read-only database connection (`engine`), we can implement the core Text-to-SQL logic. This step involves sending the user's question and the database schema to the Large Language Model (LLM) and receiving a generated SQL query.\n",
        "\n",
        "### The System Prompt\n",
        "\n",
        "The **system prompt** is a critical piece of the pipeline. It defines the LLM's role, constraints, and output format. Key instructions include:\n",
        "\n",
        "* **Role Definition:** \"You convert natural-language questions into read-only SQLite SQL.\"\n",
        "* **Safety Guardrails:** Enforcing `SELECT` queries only, blocking all data definition/manipulation language (DDL/DML) commands (`INSERT`, `UPDATE`, `DELETE`, etc.).\n",
        "* **Format Constraint:** Using `response_format={\"type\": \"json_object\"}` ensures the output is consistently parseable, reducing hallucination of the surrounding text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dadeb363",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sql_openai(question: str, schema: str, model: str = None) -> str:\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    _SYSTEM_PROMPT = \"\"\"You convert natural-language questions into read-only SQLite SQL.\n",
        "                        Rules:\n",
        "                        - Output only a JSON object like: {\"sql\": \"...\"}.\n",
        "                        - Use SELECT queries only. Never write INSERT/UPDATE/DELETE/DDL.\n",
        "                        - Prefer clear column aliases and LIMIT if the result could be large.\n",
        "                        - The target dialect is SQLite.\n",
        "                    \"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\": _SYSTEM_PROMPT},\n",
        "            {\"role\":\"user\",\"content\": f\"schema:\\n{schema}\\n\\nquestion: {question}\"},\n",
        "        ],\n",
        "    )\n",
        "    payload = json.loads(resp.choices[0].message.content)\n",
        "    return payload[\"sql\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2228eb5b",
      "metadata": {},
      "source": [
        "## Executing the Generated SQL\n",
        "\n",
        "Once the LLM successfully translates the natural language question into a `SELECT` query, the next critical step is safely executing it against the database. We use the `run_sql` function for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d124a694",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_sql(engine: Engine, sql: str) -> pd.DataFrame:\n",
        "    with engine.connect() as conn:\n",
        "        df = pd.read_sql_query(sql, conn)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8ab0d7",
      "metadata": {},
      "source": [
        "## 🚀 Step 3 — Test the Baseline Pipeline\n",
        "\n",
        "With the generation and execution functions ready, we can run our first complete Text-to-SQL query.\n",
        "\n",
        "### 3.1 Initial Test (Count Query)\n",
        "\n",
        "We'll start with a simple query that requires only one table, confirming the LLM can correctly select the table and aggregate data based on the prompt and the provided schema string (`SCHEMA_STR`).\n",
        "\n",
        "Use the foollowing simple test question and first apply the `generate_sql_openai` function and then use the output for the `run_sql` function.\n",
        "\n",
        "`\"How many customers do we have?\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "1c207767",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SELECT COUNT(*) AS total_customers FROM customers;'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sql_openai(\"How many customers do we have?\", SCHEMA_STR,\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "697c44e2",
      "metadata": {},
      "source": [
        "Now test the following question and observe qhether it correctly links `customers` and `orders` to produce the count per customer.\n",
        "\n",
        "```python\n",
        "question = \"Show the number of orders placed by each customer.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d7b2954a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'SELECT c.id AS customer_id, c.name AS customer_name, COUNT(o.id) AS order_count FROM customers c LEFT JOIN orders o ON c.id = o.customer_id GROUP BY c.id, c.name LIMIT 100;'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sql_openai(\"show the number of orders placed by each customer\", SCHEMA_STR,\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea908ad",
      "metadata": {},
      "source": [
        "## 🖼️ Step 4 — Formatting Results for the LLM Answer\n",
        "\n",
        "The `run_sql` function returns a raw **Pandas DataFrame**, which is great for computation but not ideal for direct text input to another Large Language Model (LLM). For the **Answer Generation** stage, we need a clean, structured text representation of the results.\n",
        "\n",
        "The `preview_rows_for_prompt` function is designed to convert the DataFrame into a concise, LLM-friendly **Markdown table** string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "08b2223c",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def preview_rows_for_prompt(df: pd.DataFrame, max_rows: int = 30, max_cols: int = 12) -> str:\n",
        "    \"\"\"Turn a (small) slice of the DataFrame into a compact markdown table for the LLM prompt.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return \"(no rows)\"\n",
        "    # Trim super-wide tables\n",
        "    df_small = df.copy()\n",
        "    if df_small.shape[1] > max_cols:\n",
        "        df_small = df_small.iloc[:, :max_cols]\n",
        "    # Show at most N rows\n",
        "    df_small = df_small.head(max_rows)\n",
        "    # Convert to nice markdown table without index\n",
        "    return df_small.to_markdown(index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e70384",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   customer_id | customer_name   |   order_count |\n",
            "|--------------:|:----------------|--------------:|\n",
            "|             1 | Alice           |             2 |\n",
            "|             2 | Bob             |             1 |\n",
            "|             3 | Choi            |             1 |\n",
            "|             4 | Dara            |             0 |\n"
          ]
        }
      ],
      "source": [
        "# Assuming df or  df_2 from the previous step is still available\n",
        "# question = \"Show the number of orders placed by each customer.\"\n",
        "\n",
        "# TODO (students): Call the function on df_2 or df_2 and print the resulting string.\n",
        "\n",
        "question = \"Show the number of orders placed by each customer.\"\n",
        "sql_query = generate_sql_openai(question, SCHEMA_STR, \"gpt-4o-mini\")\n",
        "\n",
        "# SQL gegen DB ausführen und DataFrame erhalten\n",
        "df_2 = run_sql(engine, sql_query)\n",
        "\n",
        "# Ergebnis anzeigen\n",
        "print(preview_rows_for_prompt(df_2, 30, 12))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c24de4",
      "metadata": {},
      "source": [
        "## 🗣️ Step 5 — Constructing the Answer Prompt (Grounding)\n",
        "\n",
        "After successfully executing the SQL query, the final step in the Text-to-SQL pipeline is using a second LLM call to translate the raw data back into a natural language answer.\n",
        "\n",
        "This step is called **Answer Grounding** because the LLM is explicitly instructed to base its response **only** on the injected facts (the DataFrame rows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "46806cf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_answer_prompt(question: str, sql: str, df: pd.DataFrame) -> list[dict]:\n",
        "    rows_md = preview_rows_for_prompt(df)\n",
        "    ANSWER_SYSTEM = \"\"\"You are a helpful data assistant.\n",
        "                        Given a user's question, the SQL that was executed (read-only), and the resulting rows,\n",
        "                        write a concise factual answer in natural language.\n",
        "\n",
        "                        Rules:\n",
        "                        - Only use the values that actually appear in the provided rows.\n",
        "                        - If the result looks incomplete or ambiguous, say so briefly and suggest a clarifying follow-up.\n",
        "                        - Prefer bullet points or a short paragraph. Keep it tight.\n",
        "                        - If there are numbers, include them exactly as shown (do not round unless asked).\n",
        "                    \"\"\"\n",
        "    user_content = f\"\"\"Question:\n",
        "                        {question}\n",
        "\n",
        "                        SQL:\n",
        "                        {sql}\n",
        "\n",
        "                        Rows (first few shown):\n",
        "                        {rows_md}\n",
        "                    \"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": ANSWER_SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": user_content},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43bf5711",
      "metadata": {},
      "source": [
        "# 📝 Step 6 — Generating the Final Natural Language Answer\n",
        "\n",
        "This is the final step in the Text-to-SQL pipeline. The `llm_answer_openai` function takes the prompt generated in **Step 5** (which includes the question, SQL, and data rows) and sends it to the LLM to get the final, human-readable answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "42b99c8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def llm_answer_openai(question: str, sql: str, df: pd.DataFrame, model: str = None) -> str:\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "    model = model or os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "    msgs = build_answer_prompt(question, sql, df)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=0,\n",
        "        messages=msgs,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "5803c766",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The customers who have ordered the most items are:\n",
            "\n",
            "- **Alice**: 4 items\n",
            "- **Bob**: 2 items\n",
            "- **Choi**: 1 item\n",
            "\n",
            "If you need more details or additional customers, please let me know!\n",
            "- The most popular product in terms of quantity sold is the **Monitor 27\"**.\n",
            "- Total units sold: **2**\n",
            "- Total price: **700**\n"
          ]
        }
      ],
      "source": [
        "# Test the pipeline\n",
        "question_3 = \"which customers has ordered the most items?\"\n",
        "sql_query_3 = generate_sql_openai(question_3, SCHEMA_STR, \"gpt-4o-mini\")\n",
        "\n",
        "# Check\n",
        "#print(sql_query_3)\n",
        "\n",
        "# SQL gegen DB ausführen und DataFrame erhalten\n",
        "df_3 = run_sql(engine, sql_query_3)\n",
        "\n",
        "# Ergebnis anzeigen\n",
        "print(llm_answer_openai(question_3, sql_query_3, df_3, \"gpt-4o-mini\"))\n",
        "\n",
        "\n",
        "def text_to_sql_qa(question, schema, engine, model=\"gpt-4o-mini\"):\n",
        "    sql = generate_sql_openai(question, schema, model)\n",
        "    df = run_sql(engine, sql)\n",
        "    answer = llm_answer_openai(question, sql, df, model)\n",
        "    return answer\n",
        "\n",
        "print(text_to_sql_qa(\"Which product name is the most popular in terms of quantity sold, and how many units were sold and total price?\", SCHEMA_STR, engine, \"gpt-4o-mini\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next step: move snippets into a FastAPI app\n",
        "\n",
        "We'll extract the key functions from this notebook into a small app structure:\n",
        "```\n",
        "text_to_sql_app/\n",
        "  ├─ app/\n",
        "  │   ├─ main.py          # FastAPI routes\n",
        "  │   ├─ db.py            # SQLite connection helpers\n",
        "  │   ├─ prompting.py     # prompt templates & LLM call\n",
        "  │   └─ safety.py        # SQL validation & guards\n",
        "  ├─ tests/\n",
        "  │   └─ test_safety.py\n",
        "  ├─ requirements.txt\n",
        "  └─ README.md\n",
        "```\n",
        "\n",
        "You can run:\n",
        "```bash\n",
        "uvicorn app.main:app --reload\n",
        "```\n",
        "\n",
        "We'll generate a scaffold alongside this notebook so you can iterate quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb2b395",
      "metadata": {},
      "source": [
        "### Student Task: Modularize the Code\n",
        "\n",
        "Your task is to manually populate these files, ensuring you handle the imports correctly. All dependencies (like the `engine` or `SCHEMA_STR`) should be imported from `db.py` into `prompting.py` or passed as arguments to the functions where possible.\n",
        "\n",
        "#### Example: `app/main.py` Scaffold\n",
        "\n",
        "The API is already ready for use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "018a4e90",
      "metadata": {},
      "source": [
        "## 🧱 `app/database.py` — Database layer\n",
        "\n",
        "**Purpose**\n",
        "This module handles all interaction with the SQLite database: connecting, inspecting schema, and running read-only queries.\n",
        "\n",
        "**Why it matters**\n",
        "- Encapsulates database logic cleanly away from the app logic.\n",
        "- Provides schema context for grounding LLM prompts.\n",
        "\n",
        "---\n",
        "\n",
        "### 🪜 Step breakdown\n",
        "\n",
        "**1️⃣ Database setup** *(provided)*  \n",
        "- Creates a read-only SQLAlchemy engine using `demo.db`.  \n",
        "- Uses `mode=ro` to protect against accidental writes.\n",
        "\n",
        "**2️⃣ Schema extraction** *(students implement)*  \n",
        "- Use `inspect(engine)` to list tables and columns.  \n",
        "- Build a compact schema string and assign it to `SCHEMA_STR`.\n",
        "\n",
        "**3️⃣ Query execution** *(students implement)*  \n",
        "- take the function `run_sql(engine, sql)` from this notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5887444",
      "metadata": {},
      "source": [
        "## 🤖 `app/openai_utils.py` — LLM interaction layer\n",
        "\n",
        "**Purpose**\n",
        "This module contains all logic that interacts with OpenAI’s API and coordinates the\n",
        "Text→SQL→Answer pipeline. It connects natural language questions to grounded SQL results. the functions mentioned below are already in your notebook above.\n",
        "\n",
        "---\n",
        "\n",
        "### 🪜 Step breakdown\n",
        "\n",
        "**1️⃣ Data formatting** *(students implement)*\n",
        "- Implement `preview_rows_for_prompt(df, max_rows=30, max_cols=12)`  \n",
        "- Implement `build_answer_prompt(question, sql, df)`  \n",
        "  \n",
        "---\n",
        "\n",
        "**2️⃣ SQL generation** *(students implement)*\n",
        "- Implement `generate_sql_openai(question, schema, model=\"gpt-4o-mini\")`\n",
        "\n",
        "---\n",
        "\n",
        "**3️⃣ Answer generation** *(students implement)*\n",
        "- Implement `llm_answer_openai(question, sql, df, model=\"gpt-4o-mini\")`\n",
        "  - Uses `build_answer_prompt()` to provide the question, executed SQL, and preview of rows.\n",
        " \n",
        "---\n",
        "\n",
        "**4️⃣ Full pipeline** *(students implement last)*\n",
        "- Implement `text_to_sql_qa(question, schema, engine, model=\"gpt-4o-mini\")`\n",
        "  - Step 1: call `generate_sql_openai()`  \n",
        "  - Step 2: execute SQL via `run_sql(engine, sql)`  \n",
        "  - Step 3: pass results to `llm_answer_openai()`  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ddb5d30",
      "metadata": {},
      "source": [
        "## 🧩 `init_db.sql` — Initialize the demo database\n",
        "\n",
        "**Purpose**\n",
        "This file contains all the SQL statements needed to create and populate the demo database.\n",
        "It’s the same schema and data you use in the notebook, so simpply move the query into the file,\n",
        "so it can be re-run automatically when starting the app or container."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ed6cfe",
      "metadata": {},
      "source": [
        "## ▶️ Running the App Locally\n",
        "\n",
        "We already included a **minimal Streamlit frontend** so you can interact with your Text-to-SQL service through a web UI.\n",
        "\n",
        "To start everything locally, open **two terminal windows** (or two separate VS Code terminals) in your project root and run:\n",
        "\n",
        "```bash\n",
        "# Terminal 1 — Start the FastAPI backend\n",
        "uvicorn app.main:app --reload\n",
        "\n",
        "# Terminal 2 — Start the Streamlit frontend\n",
        "streamlit run app_frontend.py"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
